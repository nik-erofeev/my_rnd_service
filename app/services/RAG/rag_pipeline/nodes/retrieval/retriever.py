# import asyncio
import logging
from typing import Literal

# from langchain_community.vectorstores import OpenSearchVectorSearch
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate

# from app.services.RAG.exceptions import RagPipelineError
from app.services.RAG.llm.llm import AsyncLLM
from app.services.RAG.rag_pipeline.nodes.base.base_node import BaseNode
from app.services.RAG.rag_pipeline.state import RAGState

# from langchain_huggingface import HuggingFaceEmbeddings
# from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type


# from app.utils.logging_decorators import log_execution_time

logger = logging.getLogger(__name__)


class RetrieverIntent(BaseNode):
    """
    –£–∑–µ–ª, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ (MultiQuery) –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫
    –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ - –º–æ–∫–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è).
    """

    # –ö–û–ù–°–¢–ê–ù–¢–´ –¥–ª—è —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    SEARCH_TYPE_BM25: Literal["bm25"] = "bm25"
    SEARCH_TYPE_VECTOR: Literal["vector"] = "vector"

    # –ö–û–ù–°–¢–ê–ù–¢–´ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –≤ –º–µ—Ç–æ–¥–∞—Ç–µ, –≤ –ø—Ä–∏–º–µ—Ä–µ - verify_id
    VERIFY_ID_ALL: Literal["All"] = "All"

    def __init__(
        self,
        llm: AsyncLLM,
        prompt: str,
        ## todo: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è embedding/opensearch
        # opensearch: OpenSearchVectorSearch,
        # embedding_model: HuggingFaceEmbeddings,
        ## todo: –¥—Ä–ø –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        # k: int,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        # n: int,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∑–∞–ø—Ä–æ—Å–∞
        # relevance_threshold: float,
        # use_hybrid_search: bool,
        # bm25_weight: float,
    ):
        super().__init__()
        self.llm = llm
        self.prompt = PromptTemplate.from_template(prompt)
        # self.opensearch = opensearch
        # self.embedding_model = embedding_model
        # self.k = k
        # self.n = n
        # self.relevance_threshold = relevance_threshold
        # self.use_hybrid_search = use_hybrid_search
        # self.bm25_weight = bm25_weight

    async def ainvoke(self, state: RAGState) -> RAGState:
        """–û—Å–Ω–æ–≤–Ω–æ–π entrypoint: –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        logger.info("üîç RetrieverIntent –∑–∞–ø—É—â–µ–Ω...")

        main_query, history = self._prepare_queries(state)

        # intent_queries –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ø—Ä–∏ –≤–∫–ª—é—á–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        intent_queries = self._prepare_intent_queries(state)

        # –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—É—á–∞–µ–º –∏—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä –≤ OpenSearch)

        # ## –ø—Ä–∏–º–µ—Ä –±–µ–∑ –º–æ–∫–∞
        # # verify_id = state.get("additional_data", {}).get("verify_id")  # –Ω–∞–ø—Ä–∏–º–µ—Ä –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        # verify_id = "All"  # –Ω–∞–ø—Ä–∏–º–µ—Ä –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        # retrieved = await self._execute_queries(
        #     main_query=main_query,
        #     intent_queries=intent_queries,
        #     history=history,
        #     verify_id=verify_id,
        # )

        # ## todo: –ø—Ä–∏–º–µ—Ä —Å –º–æ–∫–æ–º!
        prompt = self.prompt.format(message=main_query, history=history)
        response = await self.llm.generate([{"role": "user", "text": str(prompt)}])
        logger.info(f"üîç LLM –æ—Ç–≤–µ—Ç–∏–ª: {response.alternatives[-1].message.text}")
        mock_result: list[Document] = [
            Document(
                page_content=f"–ö–∞–∫–æ–π-—Ç–æ —Ç–µ–∫—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π_{i}",
                metadata={
                    "id": str(i),
                    "AdditionalData": {"parentName": f"parentName_{i}"},
                },
            )
            for i in range(1, 4)
        ]
        retrieved = mock_result

        unique_docs = self._deduplicate_docs(retrieved)
        return {"retrieved": unique_docs}

    def _prepare_queries(self, state: RAGState) -> tuple[str, list[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –∏ –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π."""
        messages = state["messages"]
        main_query = self.process_input_message(messages[-1])
        history = self.process_input_list(messages[:-1])
        return main_query, history

    def _prepare_intent_queries(self, state: RAGState) -> list[str]:
        """–°–ø–∏—Å–æ–∫ intent-–∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞."""
        last_intent = self._extract_last_intent(state.get("intent"))
        return [last_intent] if last_intent else []

    @staticmethod
    def _extract_last_intent(intent_data: list | None) -> str | None:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π intent –∫–∞–∫ —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å."""
        if not intent_data:
            return None
        return str(intent_data[-1])

    def _deduplicate_docs(self, docs: list[Document]) -> list[Document]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã."""
        unique_docs = self.make_chunks_unique(docs)
        logger.info(f"üìÑ –ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(unique_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤(–∞)")
        return unique_docs

    @staticmethod
    def make_chunks_unique(chunks: list[Document]) -> list[Document]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É."""
        seen = set()
        unique = []
        for chunk in chunks:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            key = chunk.page_content[:200]
            if key not in seen:
                seen.add(key)
                unique.append(chunk)
        return unique

    # ## todo: –Ω–∏–∂–µ –ø—Ä–∏–º–µ—Ä –±–µ–∑ –º–æ–∫–∞ - —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ç—å
    # async def _execute_queries(
    #     self,
    #     main_query: str,
    #     intent_queries: list[str],
    #     history: list[str],
    #     verify_id: list[str] | str,
    # ) -> list[Document]:
    #     """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –∏ intent-–∑–∞–ø—Ä–æ—Å–∞–º."""
    #     retrieved = await self._a_retrieve_multi(main_query, history, verify_id)
    #     for intent_query in intent_queries:
    #         retrieved.extend(await self._a_retrieve_multi(intent_query, history, verify_id))
    #         logger.info("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ intent –≤—ã–ø–æ–ª–Ω–µ–Ω")
    #     return retrieved
    #
    # @log_execution_time
    # async def _a_retrieve_multi(
    #     self,
    #     message: str,
    #     history: list[str] | None,
    #     verify_id: list[str] | str,
    # ) -> list[Document]:
    #     """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏ –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É."""
    #     history = history or []
    #
    #     filter_clause = self._build_filter_clause(verify_id)
    #
    #     vector_docs: list[Document] = []
    #     bm25_docs: list[Document] = []
    #
    #     for attempt in range(self.n):
    #         try:
    #             llm_query = await self._generate_rewritten_query(message, history)
    #
    #             vector_docs, bm_docs = await self._search_once(
    #                 query=llm_query,
    #                 verify_id=verify_id,
    #                 filter_clause=filter_clause,
    #             )
    #
    #             vector_docs.extend(vector_docs)
    #             bm25_docs.extend(bm_docs)
    #
    #             self._log_iteration_stats(
    #                 attempt=attempt,
    #                 query=llm_query,
    #                 vector_docs=vector_docs,
    #                 bm25_docs=bm_docs,
    #             )
    #
    #         except RagPipelineError:
    #             raise
    #         except Exception as e:
    #             raise RagPipelineError(
    #                 message=f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenSearch: {e!r}",
    #             ) from e
    #
    #     all_docs = self._merge_search_results(vector_docs=vector_docs, bm25_docs=bm25_docs)
    #     self._log_pre_dedup_stats(all_docs)
    #
    #     return all_docs
    #
    # async def _generate_rewritten_query(
    #     self,
    #     message: str,
    #     history: list[str],
    # ) -> str:
    #     prompt = self.prompt.format(message=message, history=history)
    #     response = await self.llm.generate(
    #         [{"role": "user", "text": str(prompt)}],
    #     )
    #     return response.alternatives[-1].message.text
    #
    # async def _search_once(
    #     self,
    #     query: str,
    #     verify_id: list[str] | str,
    #     filter_clause: dict | None,
    # ) -> tuple[list[Document], list[Document]]:
    #     """
    #     –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω –ø–æ–∏—Å–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É.
    #     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    #       - list[Document] –∏–∑ vector-–ø–æ–∏—Å–∫–∞
    #       - list[Document] –∏–∑ bm25 (–ø—É—Å—Ç–æ–π, –µ—Å–ª–∏ hybrid –æ—Ç–∫–ª—é—á—ë–Ω)
    #     """
    #     if not self.use_hybrid_search:
    #         vector_docs = await self._vector_search(query, verify_id, filter_clause)
    #         return vector_docs, []
    #
    #     return await asyncio.gather(
    #         self._vector_search(query, verify_id, filter_clause),
    #         self._bm25_search(query, verify_id, filter_clause),
    #     )
    #
    # def _log_iteration_stats(
    #     self,
    #     attempt: int,
    #     query: str,
    #     vector_docs: list[Document],
    #     bm25_docs: list[Document],
    # ) -> None:
    #     suffix = (
    #         f"{len(vector_docs)} vector + {len(bm25_docs)} bm25"
    #         if self.use_hybrid_search
    #         else f"{len(vector_docs)} vector"
    #     )
    #
    #     logger.info(f"üîç [{attempt + 1}] –ó–∞–ø—Ä–æ—Å '{query}' ‚Äî –Ω–∞–π–¥–µ–Ω–æ: {suffix}")
    #
    # @staticmethod
    # def _merge_search_results(
    #     vector_docs: list[Document],
    #     bm25_docs: list[Document],
    # ) -> list[Document]:
    #     # –í —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
    #     # –µ—Å–ª–∏ vector-–ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç
    #     if not vector_docs:
    #         return []
    #
    #     return vector_docs + bm25_docs
    #
    # def _log_pre_dedup_stats(self, docs: list[Document]) -> None:
    #     if not docs:
    #         return
    #
    #     stats: dict[str, int] = {}
    #     for doc in docs:
    #         search_type = doc.metadata.get("_search_type", "unknown")
    #         stats[search_type] = stats.get(search_type, 0) + 1
    #
    #     logger.info(
    #         f"üìä –î–û –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ (–ø–æ —Ç–∏–ø–∞–º –ø–æ–∏—Å–∫–∞): {stats}\n"
    #         f"   üíæ –ß–∞–Ω–∫–∏:\n{self._format_docs_info(docs, format_json=False)}",
    #     )
    #
    # @staticmethod
    # def _format_docs_info(docs: list[Document], format_json: bool = True) -> str:
    #     """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
    #
    #     Args:
    #         docs: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    #         format_json: –ï—Å–ª–∏ True - –≤—ã–≤–æ–¥–∏—Ç AdditionalData –≤ –∫—Ä–∞—Å–∏–≤–æ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ,
    #                     –µ—Å–ª–∏ False - –≤—ã–≤–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –±–µ–∑ JSON
    #
    #     Returns:
    #         str: –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —á–∞–Ω–∫–∞—Ö
    #     """
    #     if not docs:
    #         return "–Ω–µ—Ç —á–∞–Ω–∫–æ–≤"
    #
    #     docs_info: list[str] = []
    #
    #     if format_json:
    #         # –° JSON —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º (–ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥)
    #         for i, doc in enumerate(docs, 1):
    #             # –ü–æ–ª—É—á–∞–µ–º –í–°–Æ AdditionalData –∏–∑ metadata
    #             additional_data = doc.metadata.get("AdditionalData", {})
    #             score = doc.metadata.get("_score", "N/A")
    #             search_type = doc.metadata.get("_search_type", "unknown")
    #
    #             # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º AdditionalData –∫–∞–∫ –∫—Ä–∞—Å–∏–≤–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON
    #             additional_data_json = json.dumps(additional_data, ensure_ascii=False, indent=2)
    #
    #             docs_info.append(f"  [{i}] score={score} | type={search_type}")
    #             docs_info.append("      AdditionalData:")
    #             # –î–æ–±–∞–≤–ª—è–µ–º JSON —Å –æ—Ç—Å—Ç—É–ø–æ–º
    #             for line in additional_data_json.split("\n"):
    #                 docs_info.append(f"        {line}")
    #     else:
    #         # –ë–ï–ó JSON —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥)
    #         for i, doc in enumerate(docs, 1):
    #             score = doc.metadata.get("_score", "N/A")
    #             search_type = doc.metadata.get("_search_type", "unknown")
    #
    #             # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –∏–∑ AdditionalData
    #             additional_data = doc.metadata.get("AdditionalData", {})
    #             card_id = additional_data.get("cardId", "N/A")
    #
    #             docs_info.append(
    #                 f"  [{i}] score={score} | type={search_type} | cardId={card_id} | AdditionalData={additional_data}",  # noqa: E501
    #             )
    #
    #     return "\n".join(docs_info)
    #
    # @log_execution_time
    # async def _vector_search(
    #     self,
    #     query_text: str,
    #     verify_id: list[str] | str,
    #     filter_clause: dict | None = None,
    # ) -> list[Document]:
    #     """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫."""
    #     return await self._search(
    #         mode=self.SEARCH_TYPE_VECTOR,
    #         query_text=query_text,
    #         verify_id=verify_id,
    #         filter_clause=filter_clause,
    #     )
    #
    # async def _search(
    #     self,
    #     mode: str,
    #     query_text: str,
    #     verify_id: list[str] | str,
    #     filter_clause: dict | None = None,
    # ) -> list[Document]:
    #     """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ BM25 –∏–ª–∏ Vector."""
    #     try:
    #         # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π filter_clause (–∏–ª–∏ —Å—Ç—Ä–æ–∏–º, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
    #         if filter_clause is None:
    #             filter_clause = self._build_filter_clause(verify_id)
    #
    #         if mode == self.SEARCH_TYPE_BM25:
    #             body = self._build_bm25_query(query_text=query_text, filter_clause=filter_clause, verify_id=verify_id)
    #             response = await self.opensearch.async_client.search(
    #                 index=self.opensearch.index_name,
    #                 body=body,
    #             )
    #             docs = self._process_search_results(response=response, search_type=self.SEARCH_TYPE_BM25)
    #
    #             logger.info(f"‚úÖ BM25: –Ω–∞–π–¥–µ–Ω–æ {len(docs)} —á–∞–Ω–∫–æ–≤(–∞)")
    #             logger.info(f"üìÑ –ß–∞–Ω–∫(–∏) BM25:\n{self._format_docs_info(docs=docs, format_json=False)}")
    #             return docs
    #
    #         if mode == self.SEARCH_TYPE_VECTOR:
    #             query_embedding = await self.embedding_model.aembed_query(query_text)
    #             body = self._build_vector_query(
    #                 query_embedding=query_embedding,
    #                 filter_clause=filter_clause,
    #                 verify_id=verify_id,
    #             )
    #             response = await self.execute_search(body=body, mode=mode)
    #             docs = self._process_search_results(response=response, search_type=self.SEARCH_TYPE_VECTOR)
    #
    #             logger.info(f"‚úÖ Vector: –Ω–∞–π–¥–µ–Ω–æ {len(docs)} —á–∞–Ω–∫–æ–≤(–∞)")
    #             logger.info(f"üìÑ –ß–∞–Ω–∫–∏ Vector:\n{self._format_docs_info(docs=docs, format_json=False)}")
    #             return docs
    #
    #         # –∏–Ω–∞—á–µ
    #         raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞: {mode}")
    #
    #     except RagPipelineError:
    #         raise
    #     except Exception as e:
    #         logger.error(f"–û—à–∏–±–∫–∞ –≤ _search ({mode}): {e}")
    #         raise RagPipelineError(
    #             message=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ ({mode}): {e!r}",
    #         ) from e
    #
    # def _build_filter_clause(self, verify_id: list[str] | str) -> dict | None:
    #     """–°—Ç—Ä–æ–∏—Ç OpenSearch filter clause –ø–æ verify_id.
    #
    #     Returns:
    #         dict: {"terms": {"metadata.AdditionalData.cardId.keyword": [...]}}
    #         None: –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –Ω–µ –Ω—É–∂–µ–Ω
    #     """
    #
    #     # –°–õ–£–ß–ê–ô 1: verify_id == "All" ‚Üí –ë–ï–ó –§–ò–õ–¨–¢–†–ê
    #     if verify_id == self.VERIFY_ID_ALL:
    #         logger.info("üìã –§–∏–ª—å—Ç—Ä –ø–æ verify_id: 'All' ‚Üí –ë–ï–ó –§–ò–õ–¨–¢–†–ê")
    #         return None
    #
    #     # –°–õ–£–ß–ê–ô 2: verify_id == ["card_1", "card_2", ...] ‚Üí –° –§–ò–õ–¨–¢–†–û–ú
    #     if isinstance(verify_id, list):
    #         filter_clause = {"terms": {"metadata.AdditionalData.cardId.keyword": verify_id}}
    #         logger.info(f"üìã –§–∏–ª—å—Ç—Ä –ø–æ verify_id: {verify_id} ‚Üí –° –§–ò–õ–¨–¢–†–û–ú")
    #         return filter_clause
    #
    #     # –°–õ–£–ß–ê–ô 3: verify_id == None –∏–ª–∏ –¥—Ä—É–≥–æ–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Üí –û–®–ò–ë–ö–ê
    #     else:
    #         logger.error(f"‚ùå verify_id –∏–º–µ–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {verify_id}")
    #         raise RagPipelineError(
    #             message=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞: verify_id={verify_id} (–æ–∂–∏–¥–∞–µ—Ç—Å—è '{self.VERIFY_ID_ALL}' –∏–ª–∏ —Å–ø–∏—Å–æ–∫)",  # noqa: E501
    #         )
    #
    # @staticmethod
    # def _process_search_results(response: dict, search_type: str) -> list[Document]:
    #     """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ OpenSearch –≤ Document –æ–±—ä–µ–∫—Ç—ã."""
    #     return [
    #         Document(
    #             page_content=hit["_source"]["text"],
    #             metadata={
    #                 **hit["_source"].get("metadata", {}),
    #                 "_search_type": search_type,
    #                 "_score": float(hit["_score"]),
    #             },
    #         )
    #         for hit in response.get("hits", {}).get("hits", [])
    #     ]
    #
    # @retry(
    #     stop=stop_after_attempt(3),
    #     wait=wait_exponential(multiplier=1, min=0.5, max=5),
    #     retry=retry_if_exception_type(Exception),
    #     reraise=True,
    # )
    # async def execute_search(self, body: dict, mode: Literal["vector", "bm25"]) -> dict:
    #     try:
    #         return await self.opensearch.async_client.search(index=self.opensearch.index_name, body=body)
    #     except Exception as e:
    #         raise RagPipelineError(message=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ(execute_search) ({mode}): {e!r}") from e
    #
    # def _build_vector_query(
    #     self,
    #     query_embedding: list[float],
    #     filter_clause: dict | None,
    #     verify_id: list[str] | str,
    # ) -> dict:
    #     """–°—Ç—Ä–æ–∏—Ç Vector KNN query –¥–ª—è OpenSearch."""
    #     size = int(self.k * (1 - self.bm25_weight)) or 1  # —á—Ç–æ–±—ã size –Ω–µ —Å—Ç–∞–ª 0
    #
    #     if verify_id == self.VERIFY_ID_ALL:
    #         logger.info("üîì _build_vector_query: verify_id='All' ‚Üí query.knn –±–µ–∑ filter (–ë–ï–ó –§–ò–õ–¨–¢–†–ê)")
    #         body = {
    #             "size": size,
    #             "query": {
    #                 "knn": {
    #                     "vector_field": {
    #                         "vector": query_embedding,
    #                         "k": 4,
    #                     },
    #                 },
    #             },
    #             "min_score": self.relevance_threshold,
    #         }
    #     # –í–ê–†–ò–ê–ù–¢ 2 (–° –§–ò–õ–¨–¢–†–û–ú):
    #     else:
    #         logger.info(
    #             f"üîí _build_vector_query: verify_id={verify_id} ‚Üí bool.must(knn).filter (–° –§–ò–õ–¨–¢–†–û–ú)",
    #         )
    #         body = {
    #             "size": size,
    #             "query": {
    #                 "bool": {
    #                     "must": [
    #                         {
    #                             "knn": {
    #                                 "vector_field": {
    #                                     "vector": query_embedding,
    #                                     "k": 4,
    #                                 },
    #                             },
    #                         },
    #                     ],
    #                     "filter": [filter_clause],
    #                 },
    #             },
    #             "min_score": self.relevance_threshold,
    #         }
    #
    #     return body
    #
    # def _build_bm25_query(self, query_text: str, filter_clause: dict | None, verify_id: list[str] | str) -> dict:
    #     """–°—Ç—Ä–æ–∏—Ç BM25 query –¥–ª—è OpenSearch.
    #     Returns:
    #         dict: query body –¥–ª—è OpenSearch
    #     """
    #     size = int(self.k * self.bm25_weight)
    #
    #     if verify_id == self.VERIFY_ID_ALL:
    #         logger.info("üîì _build_bm25_query: verify_id='All' ‚Üí match query (–ë–ï–ó –§–ò–õ–¨–¢–†–ê)")
    #         return {
    #             "size": size,
    #             "query": {
    #                 "match": {
    #                     "text": query_text,
    #                 },
    #             },
    #         }
    #     else:
    #         # –° –§–ò–õ–¨–¢–†–û–ú: bool query —Å must + filter
    #         logger.info(f"üîí _build_bm25_query: verify_id={verify_id} ‚Üí bool query (–° –§–ò–õ–¨–¢–†–û–ú)")
    #         return {
    #             "size": size,
    #             "query": {
    #                 "bool": {
    #                     "must": [
    #                         {
    #                             "match": {
    #                                 "text": query_text,
    #                             },
    #                         },
    #                     ],
    #                     "filter": [filter_clause],  # ‚Üê list[dict] –û–ö –¥–ª—è bool
    #                 },
    #             },
    #         }
    #
    # @log_execution_time
    # async def _bm25_search(
    #     self,
    #     query_text: str,
    #     verify_id: list[str] | str,
    #     filter_clause: dict | None = None,
    # ) -> list[Document]:
    #     """BM25 –ø–æ–∏—Å–∫."""
    #     return await self._search(
    #         mode=self.SEARCH_TYPE_BM25,
    #         query_text=query_text,
    #         verify_id=verify_id,
    #         filter_clause=filter_clause,
    #     )
